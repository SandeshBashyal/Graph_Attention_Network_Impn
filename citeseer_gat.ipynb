{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEW_O_NzQ_O4",
        "outputId": "00d10c0d-8d50-4cbf-9fd7-728d7de16c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CiteSeer()\n",
            "number of graphs:\t\t 1\n",
            "number of classes:\t\t 6\n",
            "number of classes:\t\t [0 1 2 3 4 5]\n",
            "number of node features:\t 3703\n",
            "number of edge features:\t 0\n",
            "X shape:  torch.Size([3327, 3703])\n",
            "Edge shape:  torch.Size([2, 9104])\n",
            "Y shape:  torch.Size([3327])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!pip install torch networkx torch-geometric --quiet\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import numpy as np\n",
        "\n",
        "# seed = 42\n",
        "# torch.manual_seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# random.seed(seed)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Load the Citeseer dataset\n",
        "dataset = Planetoid(root='data/Citeseer', name='CiteSeer')\n",
        "\n",
        "# Extract the graph data object\n",
        "data = dataset[0]\n",
        "print(dataset)\n",
        "print(\"number of graphs:\\t\\t\",len(dataset))\n",
        "print(\"number of classes:\\t\\t\",dataset.num_classes)\n",
        "print(\"number of classes:\\t\\t\",np.unique(data.y))\n",
        "print(\"number of node features:\\t\",data.num_node_features)\n",
        "print(\"number of edge features:\\t\",data.num_edge_features)\n",
        "print(\"X shape: \", data.x.shape)\n",
        "print(\"Edge shape: \", data.edge_index.shape)\n",
        "print(\"Y shape: \", data.y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "import random\n",
        "import numpy as np\n",
        "# Define the GAT Model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout=0.6):\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # First GAT layer: Multi-head attention\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
        "        # Second GAT layer: Single-head attention\n",
        "        self.gat2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply dropout to input features\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # First GAT layer + ELU activation\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        # Apply dropout after first layer\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # Second GAT layer + LogSoftmax\n",
        "        x = self.gat2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Model, optimizer, and training setup\n",
        "input_dim = dataset.num_features\n",
        "hidden_dim = 8  # Number of features per attention head\n",
        "output_dim = dataset.num_classes\n",
        "heads = 8  # Number of attention heads in the first layer\n",
        "dropout = 0.6  # Dropout rate\n",
        "\n",
        "model = GAT(input_dim, hidden_dim, output_dim, heads=heads, dropout=dropout)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "# Training and Testing functions\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        correct = (pred[mask] == data.y[mask]).sum()\n",
        "        accs.append(int(correct) / int(mask.sum()))\n",
        "    return accs"
      ],
      "metadata": {
        "id": "nTqomzom4M4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop with best accuracy tracking\n",
        "best_val_acc = 0.0\n",
        "best_test_acc = 0.0\n",
        "\n",
        "for epoch in range(200):  # Train for 200 epochs as per the paper\n",
        "    loss = train()\n",
        "    train_acc, val_acc, test_acc = test()\n",
        "\n",
        "    # Track best validation and corresponding test accuracy\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_test_acc = test_acc\n",
        "\n",
        "    print(f'Epoch {epoch+1:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}, Best Test: {best_test_acc:.4f}')\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "760nK-Qt4Qcx",
        "outputId": "0c22fe50-ecaa-4837-8efc-17bd4d93b467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, Loss: 1.8010, Train: 0.6500, Val: 0.4520, Test: 0.4330, Best Test: 0.4330\n",
            "Epoch 002, Loss: 1.5961, Train: 0.7167, Val: 0.5200, Test: 0.4760, Best Test: 0.4760\n",
            "Epoch 003, Loss: 1.5153, Train: 0.8583, Val: 0.5720, Test: 0.5640, Best Test: 0.5640\n",
            "Epoch 004, Loss: 1.2878, Train: 0.9167, Val: 0.6440, Test: 0.6210, Best Test: 0.6210\n",
            "Epoch 005, Loss: 1.1768, Train: 0.9250, Val: 0.6620, Test: 0.6530, Best Test: 0.6530\n",
            "Epoch 006, Loss: 1.0899, Train: 0.9500, Val: 0.6680, Test: 0.6590, Best Test: 0.6590\n",
            "Epoch 007, Loss: 1.0230, Train: 0.9500, Val: 0.6720, Test: 0.6630, Best Test: 0.6630\n",
            "Epoch 008, Loss: 0.8661, Train: 0.9417, Val: 0.6640, Test: 0.6650, Best Test: 0.6630\n",
            "Epoch 009, Loss: 0.9749, Train: 0.9417, Val: 0.6640, Test: 0.6660, Best Test: 0.6630\n",
            "Epoch 010, Loss: 0.9270, Train: 0.9417, Val: 0.6600, Test: 0.6630, Best Test: 0.6630\n",
            "Epoch 011, Loss: 0.7942, Train: 0.9583, Val: 0.6580, Test: 0.6570, Best Test: 0.6630\n",
            "Epoch 012, Loss: 0.9101, Train: 0.9583, Val: 0.6580, Test: 0.6520, Best Test: 0.6630\n",
            "Epoch 013, Loss: 0.8577, Train: 0.9667, Val: 0.6640, Test: 0.6520, Best Test: 0.6630\n",
            "Epoch 014, Loss: 0.8266, Train: 0.9667, Val: 0.6620, Test: 0.6530, Best Test: 0.6630\n",
            "Epoch 015, Loss: 0.6862, Train: 0.9667, Val: 0.6640, Test: 0.6580, Best Test: 0.6630\n",
            "Epoch 016, Loss: 0.7472, Train: 0.9750, Val: 0.6660, Test: 0.6550, Best Test: 0.6630\n",
            "Epoch 017, Loss: 0.8240, Train: 0.9667, Val: 0.6680, Test: 0.6580, Best Test: 0.6630\n",
            "Epoch 018, Loss: 0.8680, Train: 0.9750, Val: 0.6680, Test: 0.6600, Best Test: 0.6630\n",
            "Epoch 019, Loss: 0.7702, Train: 0.9750, Val: 0.6700, Test: 0.6530, Best Test: 0.6630\n",
            "Epoch 020, Loss: 0.8250, Train: 0.9750, Val: 0.6720, Test: 0.6530, Best Test: 0.6630\n",
            "Epoch 021, Loss: 0.5778, Train: 0.9750, Val: 0.6700, Test: 0.6600, Best Test: 0.6630\n",
            "Epoch 022, Loss: 0.7518, Train: 0.9833, Val: 0.6760, Test: 0.6620, Best Test: 0.6620\n",
            "Epoch 023, Loss: 0.6121, Train: 0.9917, Val: 0.6720, Test: 0.6660, Best Test: 0.6620\n",
            "Epoch 024, Loss: 0.6898, Train: 0.9917, Val: 0.6760, Test: 0.6620, Best Test: 0.6620\n",
            "Epoch 025, Loss: 0.7449, Train: 0.9833, Val: 0.6740, Test: 0.6630, Best Test: 0.6620\n",
            "Epoch 026, Loss: 0.5983, Train: 0.9833, Val: 0.6660, Test: 0.6590, Best Test: 0.6620\n",
            "Epoch 027, Loss: 0.5560, Train: 0.9917, Val: 0.6560, Test: 0.6510, Best Test: 0.6620\n",
            "Epoch 028, Loss: 0.5807, Train: 0.9917, Val: 0.6520, Test: 0.6490, Best Test: 0.6620\n",
            "Epoch 029, Loss: 0.7574, Train: 0.9917, Val: 0.6440, Test: 0.6490, Best Test: 0.6620\n",
            "Epoch 030, Loss: 0.5681, Train: 0.9917, Val: 0.6400, Test: 0.6480, Best Test: 0.6620\n",
            "Epoch 031, Loss: 0.5318, Train: 0.9833, Val: 0.6400, Test: 0.6480, Best Test: 0.6620\n",
            "Epoch 032, Loss: 0.6904, Train: 0.9833, Val: 0.6460, Test: 0.6520, Best Test: 0.6620\n",
            "Epoch 033, Loss: 0.5618, Train: 0.9833, Val: 0.6500, Test: 0.6530, Best Test: 0.6620\n",
            "Epoch 034, Loss: 0.6466, Train: 0.9917, Val: 0.6560, Test: 0.6550, Best Test: 0.6620\n",
            "Epoch 035, Loss: 0.7407, Train: 0.9917, Val: 0.6540, Test: 0.6550, Best Test: 0.6620\n",
            "Epoch 036, Loss: 0.6772, Train: 0.9917, Val: 0.6560, Test: 0.6550, Best Test: 0.6620\n",
            "Epoch 037, Loss: 0.5198, Train: 0.9917, Val: 0.6600, Test: 0.6490, Best Test: 0.6620\n",
            "Epoch 038, Loss: 0.6388, Train: 0.9833, Val: 0.6620, Test: 0.6520, Best Test: 0.6620\n",
            "Epoch 039, Loss: 0.5570, Train: 0.9833, Val: 0.6680, Test: 0.6520, Best Test: 0.6620\n",
            "Epoch 040, Loss: 0.6409, Train: 0.9917, Val: 0.6680, Test: 0.6510, Best Test: 0.6620\n",
            "Epoch 041, Loss: 0.5845, Train: 0.9917, Val: 0.6660, Test: 0.6520, Best Test: 0.6620\n",
            "Epoch 042, Loss: 0.6306, Train: 0.9917, Val: 0.6760, Test: 0.6510, Best Test: 0.6620\n",
            "Epoch 043, Loss: 0.7154, Train: 0.9833, Val: 0.6780, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 044, Loss: 0.6627, Train: 0.9750, Val: 0.6760, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 045, Loss: 0.5941, Train: 0.9833, Val: 0.6720, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 046, Loss: 0.4452, Train: 0.9833, Val: 0.6720, Test: 0.6530, Best Test: 0.6570\n",
            "Epoch 047, Loss: 0.7435, Train: 0.9917, Val: 0.6680, Test: 0.6510, Best Test: 0.6570\n",
            "Epoch 048, Loss: 0.5519, Train: 0.9917, Val: 0.6640, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 049, Loss: 0.6654, Train: 0.9917, Val: 0.6600, Test: 0.6510, Best Test: 0.6570\n",
            "Epoch 050, Loss: 0.6395, Train: 0.9917, Val: 0.6560, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 051, Loss: 0.6908, Train: 0.9917, Val: 0.6480, Test: 0.6450, Best Test: 0.6570\n",
            "Epoch 052, Loss: 0.4771, Train: 0.9917, Val: 0.6460, Test: 0.6390, Best Test: 0.6570\n",
            "Epoch 053, Loss: 0.5368, Train: 0.9917, Val: 0.6480, Test: 0.6360, Best Test: 0.6570\n",
            "Epoch 054, Loss: 0.5953, Train: 0.9917, Val: 0.6460, Test: 0.6320, Best Test: 0.6570\n",
            "Epoch 055, Loss: 0.5765, Train: 0.9917, Val: 0.6420, Test: 0.6340, Best Test: 0.6570\n",
            "Epoch 056, Loss: 0.6746, Train: 0.9833, Val: 0.6440, Test: 0.6330, Best Test: 0.6570\n",
            "Epoch 057, Loss: 0.5167, Train: 0.9917, Val: 0.6460, Test: 0.6320, Best Test: 0.6570\n",
            "Epoch 058, Loss: 0.5843, Train: 0.9917, Val: 0.6460, Test: 0.6360, Best Test: 0.6570\n",
            "Epoch 059, Loss: 0.5477, Train: 0.9833, Val: 0.6440, Test: 0.6410, Best Test: 0.6570\n",
            "Epoch 060, Loss: 0.4724, Train: 0.9833, Val: 0.6460, Test: 0.6430, Best Test: 0.6570\n",
            "Epoch 061, Loss: 0.5179, Train: 0.9833, Val: 0.6500, Test: 0.6440, Best Test: 0.6570\n",
            "Epoch 062, Loss: 0.6039, Train: 0.9833, Val: 0.6500, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 063, Loss: 0.5054, Train: 0.9917, Val: 0.6580, Test: 0.6550, Best Test: 0.6570\n",
            "Epoch 064, Loss: 0.5899, Train: 0.9917, Val: 0.6520, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 065, Loss: 0.6388, Train: 0.9917, Val: 0.6560, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 066, Loss: 0.5923, Train: 0.9917, Val: 0.6580, Test: 0.6580, Best Test: 0.6570\n",
            "Epoch 067, Loss: 0.3842, Train: 1.0000, Val: 0.6580, Test: 0.6600, Best Test: 0.6570\n",
            "Epoch 068, Loss: 0.5710, Train: 1.0000, Val: 0.6560, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 069, Loss: 0.4621, Train: 1.0000, Val: 0.6560, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 070, Loss: 0.4970, Train: 1.0000, Val: 0.6520, Test: 0.6590, Best Test: 0.6570\n",
            "Epoch 071, Loss: 0.6255, Train: 1.0000, Val: 0.6560, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 072, Loss: 0.5009, Train: 1.0000, Val: 0.6560, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 073, Loss: 0.4572, Train: 1.0000, Val: 0.6560, Test: 0.6540, Best Test: 0.6570\n",
            "Epoch 074, Loss: 0.5419, Train: 1.0000, Val: 0.6560, Test: 0.6540, Best Test: 0.6570\n",
            "Epoch 075, Loss: 0.5153, Train: 1.0000, Val: 0.6580, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 076, Loss: 0.5727, Train: 1.0000, Val: 0.6580, Test: 0.6590, Best Test: 0.6570\n",
            "Epoch 077, Loss: 0.5301, Train: 1.0000, Val: 0.6600, Test: 0.6580, Best Test: 0.6570\n",
            "Epoch 078, Loss: 0.5743, Train: 1.0000, Val: 0.6640, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 079, Loss: 0.5280, Train: 1.0000, Val: 0.6640, Test: 0.6530, Best Test: 0.6570\n",
            "Epoch 080, Loss: 0.3895, Train: 1.0000, Val: 0.6640, Test: 0.6520, Best Test: 0.6570\n",
            "Epoch 081, Loss: 0.4322, Train: 1.0000, Val: 0.6660, Test: 0.6520, Best Test: 0.6570\n",
            "Epoch 082, Loss: 0.6171, Train: 1.0000, Val: 0.6620, Test: 0.6520, Best Test: 0.6570\n",
            "Epoch 083, Loss: 0.4087, Train: 1.0000, Val: 0.6640, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 084, Loss: 0.4659, Train: 1.0000, Val: 0.6660, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 085, Loss: 0.5799, Train: 0.9917, Val: 0.6680, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 086, Loss: 0.5629, Train: 0.9917, Val: 0.6620, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 087, Loss: 0.5127, Train: 0.9917, Val: 0.6580, Test: 0.6450, Best Test: 0.6570\n",
            "Epoch 088, Loss: 0.6358, Train: 0.9917, Val: 0.6620, Test: 0.6420, Best Test: 0.6570\n",
            "Epoch 089, Loss: 0.4634, Train: 0.9917, Val: 0.6600, Test: 0.6420, Best Test: 0.6570\n",
            "Epoch 090, Loss: 0.5225, Train: 0.9917, Val: 0.6600, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 091, Loss: 0.5788, Train: 1.0000, Val: 0.6580, Test: 0.6520, Best Test: 0.6570\n",
            "Epoch 092, Loss: 0.5279, Train: 1.0000, Val: 0.6580, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 093, Loss: 0.4384, Train: 1.0000, Val: 0.6560, Test: 0.6450, Best Test: 0.6570\n",
            "Epoch 094, Loss: 0.4759, Train: 1.0000, Val: 0.6560, Test: 0.6460, Best Test: 0.6570\n",
            "Epoch 095, Loss: 0.5360, Train: 1.0000, Val: 0.6560, Test: 0.6450, Best Test: 0.6570\n",
            "Epoch 096, Loss: 0.4980, Train: 1.0000, Val: 0.6520, Test: 0.6430, Best Test: 0.6570\n",
            "Epoch 097, Loss: 0.5183, Train: 1.0000, Val: 0.6560, Test: 0.6460, Best Test: 0.6570\n",
            "Epoch 098, Loss: 0.4462, Train: 1.0000, Val: 0.6580, Test: 0.6490, Best Test: 0.6570\n",
            "Epoch 099, Loss: 0.5252, Train: 1.0000, Val: 0.6520, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 100, Loss: 0.6216, Train: 1.0000, Val: 0.6520, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 101, Loss: 0.4697, Train: 1.0000, Val: 0.6500, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 102, Loss: 0.4803, Train: 1.0000, Val: 0.6460, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 103, Loss: 0.5612, Train: 1.0000, Val: 0.6540, Test: 0.6490, Best Test: 0.6570\n",
            "Epoch 104, Loss: 0.5234, Train: 1.0000, Val: 0.6680, Test: 0.6530, Best Test: 0.6570\n",
            "Epoch 105, Loss: 0.6500, Train: 1.0000, Val: 0.6600, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 106, Loss: 0.5123, Train: 1.0000, Val: 0.6620, Test: 0.6590, Best Test: 0.6570\n",
            "Epoch 107, Loss: 0.5060, Train: 1.0000, Val: 0.6640, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 108, Loss: 0.5148, Train: 1.0000, Val: 0.6620, Test: 0.6590, Best Test: 0.6570\n",
            "Epoch 109, Loss: 0.4966, Train: 1.0000, Val: 0.6620, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 110, Loss: 0.5788, Train: 1.0000, Val: 0.6620, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 111, Loss: 0.5572, Train: 1.0000, Val: 0.6620, Test: 0.6650, Best Test: 0.6570\n",
            "Epoch 112, Loss: 0.3893, Train: 1.0000, Val: 0.6600, Test: 0.6650, Best Test: 0.6570\n",
            "Epoch 113, Loss: 0.5187, Train: 1.0000, Val: 0.6560, Test: 0.6680, Best Test: 0.6570\n",
            "Epoch 114, Loss: 0.4284, Train: 1.0000, Val: 0.6580, Test: 0.6660, Best Test: 0.6570\n",
            "Epoch 115, Loss: 0.4924, Train: 1.0000, Val: 0.6540, Test: 0.6660, Best Test: 0.6570\n",
            "Epoch 116, Loss: 0.5385, Train: 1.0000, Val: 0.6520, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 117, Loss: 0.3842, Train: 1.0000, Val: 0.6560, Test: 0.6630, Best Test: 0.6570\n",
            "Epoch 118, Loss: 0.5014, Train: 1.0000, Val: 0.6620, Test: 0.6590, Best Test: 0.6570\n",
            "Epoch 119, Loss: 0.4770, Train: 1.0000, Val: 0.6620, Test: 0.6600, Best Test: 0.6570\n",
            "Epoch 120, Loss: 0.5206, Train: 1.0000, Val: 0.6620, Test: 0.6600, Best Test: 0.6570\n",
            "Epoch 121, Loss: 0.5184, Train: 1.0000, Val: 0.6600, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 122, Loss: 0.5564, Train: 1.0000, Val: 0.6600, Test: 0.6580, Best Test: 0.6570\n",
            "Epoch 123, Loss: 0.5845, Train: 1.0000, Val: 0.6540, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 124, Loss: 0.4754, Train: 1.0000, Val: 0.6500, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 125, Loss: 0.5989, Train: 1.0000, Val: 0.6500, Test: 0.6580, Best Test: 0.6570\n",
            "Epoch 126, Loss: 0.4941, Train: 1.0000, Val: 0.6520, Test: 0.6600, Best Test: 0.6570\n",
            "Epoch 127, Loss: 0.5939, Train: 1.0000, Val: 0.6500, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 128, Loss: 0.6738, Train: 1.0000, Val: 0.6500, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 129, Loss: 0.3345, Train: 1.0000, Val: 0.6520, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 130, Loss: 0.4253, Train: 1.0000, Val: 0.6520, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 131, Loss: 0.5489, Train: 1.0000, Val: 0.6520, Test: 0.6580, Best Test: 0.6570\n",
            "Epoch 132, Loss: 0.4909, Train: 1.0000, Val: 0.6540, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 133, Loss: 0.5331, Train: 1.0000, Val: 0.6560, Test: 0.6660, Best Test: 0.6570\n",
            "Epoch 134, Loss: 0.4034, Train: 1.0000, Val: 0.6560, Test: 0.6650, Best Test: 0.6570\n",
            "Epoch 135, Loss: 0.4416, Train: 1.0000, Val: 0.6540, Test: 0.6670, Best Test: 0.6570\n",
            "Epoch 136, Loss: 0.5313, Train: 1.0000, Val: 0.6540, Test: 0.6710, Best Test: 0.6570\n",
            "Epoch 137, Loss: 0.5881, Train: 1.0000, Val: 0.6540, Test: 0.6740, Best Test: 0.6570\n",
            "Epoch 138, Loss: 0.5438, Train: 1.0000, Val: 0.6560, Test: 0.6800, Best Test: 0.6570\n",
            "Epoch 139, Loss: 0.5020, Train: 1.0000, Val: 0.6560, Test: 0.6790, Best Test: 0.6570\n",
            "Epoch 140, Loss: 0.6686, Train: 1.0000, Val: 0.6600, Test: 0.6810, Best Test: 0.6570\n",
            "Epoch 141, Loss: 0.5861, Train: 1.0000, Val: 0.6600, Test: 0.6820, Best Test: 0.6570\n",
            "Epoch 142, Loss: 0.5581, Train: 1.0000, Val: 0.6600, Test: 0.6830, Best Test: 0.6570\n",
            "Epoch 143, Loss: 0.4942, Train: 1.0000, Val: 0.6540, Test: 0.6820, Best Test: 0.6570\n",
            "Epoch 144, Loss: 0.5167, Train: 1.0000, Val: 0.6580, Test: 0.6790, Best Test: 0.6570\n",
            "Epoch 145, Loss: 0.6099, Train: 1.0000, Val: 0.6620, Test: 0.6790, Best Test: 0.6570\n",
            "Epoch 146, Loss: 0.4345, Train: 1.0000, Val: 0.6600, Test: 0.6790, Best Test: 0.6570\n",
            "Epoch 147, Loss: 0.4549, Train: 1.0000, Val: 0.6600, Test: 0.6780, Best Test: 0.6570\n",
            "Epoch 148, Loss: 0.6156, Train: 1.0000, Val: 0.6560, Test: 0.6730, Best Test: 0.6570\n",
            "Epoch 149, Loss: 0.3876, Train: 1.0000, Val: 0.6560, Test: 0.6720, Best Test: 0.6570\n",
            "Epoch 150, Loss: 0.5581, Train: 1.0000, Val: 0.6540, Test: 0.6670, Best Test: 0.6570\n",
            "Epoch 151, Loss: 0.4175, Train: 1.0000, Val: 0.6540, Test: 0.6670, Best Test: 0.6570\n",
            "Epoch 152, Loss: 0.5243, Train: 1.0000, Val: 0.6580, Test: 0.6660, Best Test: 0.6570\n",
            "Epoch 153, Loss: 0.5072, Train: 1.0000, Val: 0.6600, Test: 0.6600, Best Test: 0.6570\n",
            "Epoch 154, Loss: 0.4815, Train: 1.0000, Val: 0.6620, Test: 0.6580, Best Test: 0.6570\n",
            "Epoch 155, Loss: 0.6145, Train: 1.0000, Val: 0.6620, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 156, Loss: 0.5268, Train: 1.0000, Val: 0.6600, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 157, Loss: 0.4728, Train: 1.0000, Val: 0.6600, Test: 0.6630, Best Test: 0.6570\n",
            "Epoch 158, Loss: 0.4610, Train: 1.0000, Val: 0.6660, Test: 0.6630, Best Test: 0.6570\n",
            "Epoch 159, Loss: 0.6351, Train: 1.0000, Val: 0.6700, Test: 0.6650, Best Test: 0.6570\n",
            "Epoch 160, Loss: 0.5023, Train: 1.0000, Val: 0.6680, Test: 0.6630, Best Test: 0.6570\n",
            "Epoch 161, Loss: 0.5209, Train: 1.0000, Val: 0.6660, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 162, Loss: 0.4787, Train: 1.0000, Val: 0.6660, Test: 0.6620, Best Test: 0.6570\n",
            "Epoch 163, Loss: 0.6296, Train: 1.0000, Val: 0.6660, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 164, Loss: 0.5334, Train: 1.0000, Val: 0.6620, Test: 0.6550, Best Test: 0.6570\n",
            "Epoch 165, Loss: 0.4053, Train: 1.0000, Val: 0.6600, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 166, Loss: 0.4298, Train: 1.0000, Val: 0.6520, Test: 0.6490, Best Test: 0.6570\n",
            "Epoch 167, Loss: 0.5824, Train: 1.0000, Val: 0.6540, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 168, Loss: 0.4656, Train: 1.0000, Val: 0.6500, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 169, Loss: 0.5343, Train: 1.0000, Val: 0.6460, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 170, Loss: 0.5257, Train: 1.0000, Val: 0.6480, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 171, Loss: 0.3865, Train: 1.0000, Val: 0.6480, Test: 0.6490, Best Test: 0.6570\n",
            "Epoch 172, Loss: 0.5103, Train: 1.0000, Val: 0.6560, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 173, Loss: 0.4578, Train: 1.0000, Val: 0.6500, Test: 0.6510, Best Test: 0.6570\n",
            "Epoch 174, Loss: 0.4926, Train: 1.0000, Val: 0.6580, Test: 0.6520, Best Test: 0.6570\n",
            "Epoch 175, Loss: 0.4302, Train: 1.0000, Val: 0.6500, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 176, Loss: 0.4305, Train: 1.0000, Val: 0.6500, Test: 0.6560, Best Test: 0.6570\n",
            "Epoch 177, Loss: 0.5229, Train: 1.0000, Val: 0.6480, Test: 0.6530, Best Test: 0.6570\n",
            "Epoch 178, Loss: 0.5454, Train: 1.0000, Val: 0.6460, Test: 0.6550, Best Test: 0.6570\n",
            "Epoch 179, Loss: 0.6728, Train: 1.0000, Val: 0.6500, Test: 0.6590, Best Test: 0.6570\n",
            "Epoch 180, Loss: 0.4429, Train: 1.0000, Val: 0.6500, Test: 0.6600, Best Test: 0.6570\n",
            "Epoch 181, Loss: 0.5421, Train: 1.0000, Val: 0.6520, Test: 0.6610, Best Test: 0.6570\n",
            "Epoch 182, Loss: 0.4547, Train: 1.0000, Val: 0.6540, Test: 0.6570, Best Test: 0.6570\n",
            "Epoch 183, Loss: 0.5146, Train: 1.0000, Val: 0.6540, Test: 0.6540, Best Test: 0.6570\n",
            "Epoch 184, Loss: 0.5718, Train: 1.0000, Val: 0.6580, Test: 0.6490, Best Test: 0.6570\n",
            "Epoch 185, Loss: 0.3822, Train: 1.0000, Val: 0.6540, Test: 0.6490, Best Test: 0.6570\n",
            "Epoch 186, Loss: 0.4444, Train: 1.0000, Val: 0.6560, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 187, Loss: 0.4282, Train: 1.0000, Val: 0.6540, Test: 0.6480, Best Test: 0.6570\n",
            "Epoch 188, Loss: 0.5621, Train: 1.0000, Val: 0.6580, Test: 0.6460, Best Test: 0.6570\n",
            "Epoch 189, Loss: 0.4813, Train: 1.0000, Val: 0.6580, Test: 0.6420, Best Test: 0.6570\n",
            "Epoch 190, Loss: 0.4920, Train: 1.0000, Val: 0.6620, Test: 0.6390, Best Test: 0.6570\n",
            "Epoch 191, Loss: 0.5381, Train: 1.0000, Val: 0.6660, Test: 0.6440, Best Test: 0.6570\n",
            "Epoch 192, Loss: 0.6544, Train: 1.0000, Val: 0.6620, Test: 0.6430, Best Test: 0.6570\n",
            "Epoch 193, Loss: 0.5896, Train: 1.0000, Val: 0.6620, Test: 0.6460, Best Test: 0.6570\n",
            "Epoch 194, Loss: 0.6145, Train: 1.0000, Val: 0.6600, Test: 0.6470, Best Test: 0.6570\n",
            "Epoch 195, Loss: 0.4965, Train: 1.0000, Val: 0.6620, Test: 0.6500, Best Test: 0.6570\n",
            "Epoch 196, Loss: 0.4480, Train: 1.0000, Val: 0.6660, Test: 0.6510, Best Test: 0.6570\n",
            "Epoch 197, Loss: 0.5180, Train: 1.0000, Val: 0.6660, Test: 0.6520, Best Test: 0.6570\n",
            "Epoch 198, Loss: 0.4838, Train: 1.0000, Val: 0.6680, Test: 0.6540, Best Test: 0.6570\n",
            "Epoch 199, Loss: 0.4233, Train: 1.0000, Val: 0.6780, Test: 0.6550, Best Test: 0.6570\n",
            "Epoch 200, Loss: 0.5250, Train: 1.0000, Val: 0.6760, Test: 0.6580, Best Test: 0.6570\n",
            "Best Validation Accuracy: 0.6780\n",
            "Best Test Accuracy: 0.6570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "# Load the Citeseer dataset\n",
        "dataset = Planetoid(root=\"data/Citeseer\", name=\"Citeseer\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Feature normalization (row-normalize)\n",
        "data.x = data.x / data.x.sum(dim=1, keepdim=True).clamp(min=1e-10)\n",
        "\n",
        "# Define GAT Model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, dropout_rate):\n",
        "        super(GAT, self).__init__()\n",
        "        # First GAT layer with multiple attention heads\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout_rate)\n",
        "        # Second GAT layer for classification\n",
        "        self.gat2 = GATConv(hidden_dim * num_heads, output_dim, heads=1, concat=False, dropout=dropout_rate)\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.relu(x)  # First layer activation\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = dataset.num_node_features      # Number of input features\n",
        "hidden_dim = 8                             # Features per attention head\n",
        "output_dim = dataset.num_classes           # Number of classes\n",
        "num_heads = 8                              # Number of attention heads\n",
        "dropout_rate = 0.6                         # Dropout rate\n",
        "weight_decay = 5e-4                        # L2 regularization factor\n",
        "learning_rate = 0.01                       # Learning rate\n",
        "max_epochs = 200                           # Maximum epochs\n",
        "early_stopping_patience = 10               # Early stopping window size\n",
        "\n",
        "# Initialize model\n",
        "model = GAT(input_dim, hidden_dim, output_dim, num_heads, dropout_rate)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Xavier initialization for GCNConv weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, GATConv):\n",
        "        torch.nn.init.xavier_uniform_(m.lin.weight)  # Initialize the linear transformation weights\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)  # Initialize bias to zeros\n",
        "\n",
        "# Apply the initialization\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        correct = (pred[mask] == data.y[mask]).sum()\n",
        "        accs.append(int(correct) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_acc = 0\n",
        "best_test_acc = 0\n",
        "patience = 0\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    loss = train()\n",
        "    train_acc, val_acc, test_acc = evaluate()\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_test_acc = test_acc\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    if patience == early_stopping_patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"Corresponding Test Accuracy: {best_test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "b7mVUJtT4a-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0289544c-9f75-4c6e-9cb8-5e6ab30f5ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, Loss: 1.7920, Train: 0.2667, Val: 0.2560, Test: 0.2140\n",
            "Epoch 002, Loss: 1.7880, Train: 0.3250, Val: 0.3140, Test: 0.2880\n",
            "Epoch 003, Loss: 1.7833, Train: 0.4750, Val: 0.3260, Test: 0.2950\n",
            "Epoch 004, Loss: 1.7801, Train: 0.8417, Val: 0.5920, Test: 0.5680\n",
            "Epoch 005, Loss: 1.7676, Train: 0.7667, Val: 0.5520, Test: 0.5550\n",
            "Epoch 006, Loss: 1.7703, Train: 0.7417, Val: 0.5240, Test: 0.5220\n",
            "Epoch 007, Loss: 1.7549, Train: 0.7667, Val: 0.5520, Test: 0.5540\n",
            "Epoch 008, Loss: 1.7453, Train: 0.8083, Val: 0.5660, Test: 0.5740\n",
            "Epoch 009, Loss: 1.7421, Train: 0.8667, Val: 0.5860, Test: 0.5860\n",
            "Epoch 010, Loss: 1.7290, Train: 0.8917, Val: 0.6280, Test: 0.6230\n",
            "Epoch 011, Loss: 1.7206, Train: 0.9000, Val: 0.6520, Test: 0.6690\n",
            "Epoch 012, Loss: 1.7104, Train: 0.9167, Val: 0.6960, Test: 0.7120\n",
            "Epoch 013, Loss: 1.6889, Train: 0.9167, Val: 0.7220, Test: 0.7210\n",
            "Epoch 014, Loss: 1.6789, Train: 0.9167, Val: 0.7040, Test: 0.7260\n",
            "Epoch 015, Loss: 1.6787, Train: 0.9000, Val: 0.7060, Test: 0.7200\n",
            "Epoch 016, Loss: 1.6964, Train: 0.9000, Val: 0.6820, Test: 0.6870\n",
            "Epoch 017, Loss: 1.6456, Train: 0.9000, Val: 0.6660, Test: 0.6600\n",
            "Epoch 018, Loss: 1.6120, Train: 0.8833, Val: 0.6680, Test: 0.6550\n",
            "Epoch 019, Loss: 1.6214, Train: 0.8917, Val: 0.6700, Test: 0.6740\n",
            "Epoch 020, Loss: 1.6010, Train: 0.9083, Val: 0.6860, Test: 0.6860\n",
            "Epoch 021, Loss: 1.6523, Train: 0.9000, Val: 0.6880, Test: 0.7000\n",
            "Epoch 022, Loss: 1.5889, Train: 0.9083, Val: 0.7040, Test: 0.6930\n",
            "Epoch 023, Loss: 1.5591, Train: 0.9250, Val: 0.6900, Test: 0.6900\n",
            "Early stopping!\n",
            "Best Validation Accuracy: 0.7220\n",
            "Corresponding Test Accuracy: 0.7210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MapTeQt1t4eB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}